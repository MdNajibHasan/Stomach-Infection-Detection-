{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8570db0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T11:23:50.029906Z",
     "iopub.status.busy": "2024-02-21T11:23:50.029542Z",
     "iopub.status.idle": "2024-02-21T11:24:11.861297Z",
     "shell.execute_reply": "2024-02-21T11:24:11.860218Z"
    },
    "id": "et-qp0FxIEnI",
    "papermill": {
     "duration": 21.849042,
     "end_time": "2024-02-21T11:24:11.863576",
     "exception": false,
     "start_time": "2024-02-21T11:23:50.014534",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-21 11:23:54.638601: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-02-21 11:23:54.638732: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-02-21 11:23:54.807489: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "import seaborn as sns\n",
    "from skimage.filters import sobel\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers,models,Model\n",
    "from keras.models import model_from_json\n",
    "from keras.models import load_model\n",
    "\n",
    "#For VGG16\n",
    "import pathlib\n",
    "import random\n",
    "\n",
    "\n",
    "from keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.layers import Dense, Dropout, Flatten, Input, Conv2D\n",
    "from tensorflow.keras.layers import GlobalMaxPooling2D, MaxPooling2D\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "import torch\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "000c31e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T11:24:11.892357Z",
     "iopub.status.busy": "2024-02-21T11:24:11.891627Z",
     "iopub.status.idle": "2024-02-21T11:24:26.598387Z",
     "shell.execute_reply": "2024-02-21T11:24:26.597170Z"
    },
    "id": "eq457vJaIFkq",
    "outputId": "3a65db39-d358-4cf1-b6c9-5df61fb011e1",
    "papermill": {
     "duration": 14.723678,
     "end_time": "2024-02-21T11:24:26.600983",
     "exception": false,
     "start_time": "2024-02-21T11:24:11.877305",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting visualkeras\r\n",
      "  Downloading visualkeras-0.0.2-py3-none-any.whl (12 kB)\r\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from visualkeras) (9.5.0)\r\n",
      "Requirement already satisfied: numpy>=1.18.1 in /opt/conda/lib/python3.10/site-packages (from visualkeras) (1.24.4)\r\n",
      "Collecting aggdraw>=1.3.11 (from visualkeras)\r\n",
      "  Downloading aggdraw-1.3.18-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (655 bytes)\r\n",
      "Downloading aggdraw-1.3.18-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (993 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m993.7/993.7 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: aggdraw, visualkeras\r\n",
      "Successfully installed aggdraw-1.3.18 visualkeras-0.0.2\r\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "import cv2\n",
    "import os\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "warnings.filterwarnings('ignore')\n",
    "!pip install visualkeras\n",
    "import visualkeras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4915c42c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T11:24:26.631354Z",
     "iopub.status.busy": "2024-02-21T11:24:26.630983Z",
     "iopub.status.idle": "2024-02-21T11:24:26.637770Z",
     "shell.execute_reply": "2024-02-21T11:24:26.636893Z"
    },
    "id": "gClx4X2GIHsg",
    "papermill": {
     "duration": 0.024325,
     "end_time": "2024-02-21T11:24:26.639748",
     "exception": false,
     "start_time": "2024-02-21T11:24:26.615423",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, MaxPool2D, Dropout, Flatten, BatchNormalization\n",
    "from keras.applications.efficientnet import EfficientNetB3,EfficientNetB2,EfficientNetB0,EfficientNetB6,EfficientNetB5,EfficientNetB4\n",
    "from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "from keras.applications.mobilenet_v2 import MobileNetV2\n",
    "from tensorflow.keras import regularizers\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.optimizers import Adam, Adamax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9c8f7db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T11:24:26.670993Z",
     "iopub.status.busy": "2024-02-21T11:24:26.670267Z",
     "iopub.status.idle": "2024-02-21T11:24:26.678575Z",
     "shell.execute_reply": "2024-02-21T11:24:26.677864Z"
    },
    "id": "_H2dXbeFIPXA",
    "papermill": {
     "duration": 0.0267,
     "end_time": "2024-02-21T11:24:26.680562",
     "exception": false,
     "start_time": "2024-02-21T11:24:26.653862",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from keras.layers import concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.applications.xception import Xception\n",
    "#from tensorflow.keras.applications.xception import layers\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv2D, Conv3D, SeparableConv2D, MaxPooling2D, BatchNormalization, GlobalAveragePooling2D, Dense, Add, Activation, AveragePooling2D\n",
    "from tensorflow.keras.activations import gelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c72c8174",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T11:24:26.711270Z",
     "iopub.status.busy": "2024-02-21T11:24:26.710983Z",
     "iopub.status.idle": "2024-02-21T11:24:26.717045Z",
     "shell.execute_reply": "2024-02-21T11:24:26.716084Z"
    },
    "id": "lSMOuRLvyo9q",
    "papermill": {
     "duration": 0.02392,
     "end_time": "2024-02-21T11:24:26.719112",
     "exception": false,
     "start_time": "2024-02-21T11:24:26.695192",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def DepthwiseSeparable_gelu0(input_layer):\n",
    "    # input_layer = Input(shape=input_shape)\n",
    "\n",
    "    # Block 1\n",
    "#     x = DepthwiseConv2D(3, padding='same')(input_layer)\n",
    "#     x = SeparableConv2D(64, (3, 3), padding='same')(x)\n",
    "#     x = Activation(gelu)(x)\n",
    "# #     #x = MaxPooling2D((2, 2), strides=1)(x)\n",
    "\n",
    "# #     # Block 2\n",
    "#     x = DepthwiseConv2D(3, padding='same')(x)\n",
    "#     x = SeparableConv2D(128, (3, 3), padding='same')(x)\n",
    "#     x = Activation(gelu)(x)\n",
    "#     #x = MaxPooling2D((2, 2), strides=1)(x)\n",
    "\n",
    "    # Block 3\n",
    "    x = DepthwiseConv2D(3, padding='same')(input_layer)\n",
    "    x = SeparableConv2D(256, (3, 3), padding='same')(x)\n",
    "    x = Activation(gelu)(x)\n",
    "    x = SeparableConv2D(256, (3, 3), padding='same')(x)\n",
    "    x = Activation(gelu)(x)\n",
    "    #x = MaxPooling2D((2, 2), strides=1)(x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f2600f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T11:24:26.750020Z",
     "iopub.status.busy": "2024-02-21T11:24:26.749749Z",
     "iopub.status.idle": "2024-02-21T11:24:26.757365Z",
     "shell.execute_reply": "2024-02-21T11:24:26.756395Z"
    },
    "id": "oElQdaiBxjYP",
    "papermill": {
     "duration": 0.026045,
     "end_time": "2024-02-21T11:24:26.759696",
     "exception": false,
     "start_time": "2024-02-21T11:24:26.733651",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, DepthwiseConv2D, SeparableConv2D, MaxPooling2D, Flatten, Dense, Activation\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "\n",
    "# Define the VGG16 architecture with Depthwise Separable Convolution and GELU activation\n",
    "def DepthwiseSeparable_gelu1(input_layer):\n",
    "    # input_layer = Input(shape=input_shape)\n",
    "\n",
    "    # Block 1\n",
    "#     x = DepthwiseConv2D(3, padding='same')(input_layer)\n",
    "#     x = SeparableConv2D(64, (3, 3), padding='same')(x)\n",
    "#     x = Activation(gelu)(x)\n",
    "# #     #x = MaxPooling2D((2, 2), strides=(2, 2))(x)\n",
    "\n",
    "# #     # Block 2\n",
    "#     x = DepthwiseConv2D(3, padding='same')(x)\n",
    "#     x = SeparableConv2D(128, (3, 3), padding='same')(x)\n",
    "#     x = Activation(gelu)(x)\n",
    "#     #x = MaxPooling2D((2, 2), strides=(2, 2))(x)\n",
    "\n",
    "#     # Block 3\n",
    "#     x = DepthwiseConv2D(3, padding='same')(x)\n",
    "#     x = SeparableConv2D(256, (3, 3), padding='same')(x)\n",
    "#     x = Activation(gelu)(x)\n",
    "#     x = SeparableConv2D(256, (3, 3), padding='same')(x)\n",
    "#     x = Activation(gelu)(x)\n",
    "    #x = MaxPooling2D((2, 2), strides=(2, 2))(x)\n",
    "\n",
    "    # Block 4\n",
    "    x = DepthwiseConv2D(3, padding='same')(input_layer)\n",
    "    x = SeparableConv2D(512, (3, 3), padding='same')(x)\n",
    "    x = Activation(gelu)(x)\n",
    "    x = SeparableConv2D(512, (3, 3), padding='same')(x)\n",
    "    x = Activation(gelu)(x)\n",
    "    #x = MaxPooling2D((2, 2), strides=(2, 2))(x)\n",
    "\n",
    "\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69875f7e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T11:24:26.792326Z",
     "iopub.status.busy": "2024-02-21T11:24:26.791713Z",
     "iopub.status.idle": "2024-02-21T11:24:26.799112Z",
     "shell.execute_reply": "2024-02-21T11:24:26.798253Z"
    },
    "id": "LnleZFICwLKt",
    "papermill": {
     "duration": 0.025938,
     "end_time": "2024-02-21T11:24:26.801310",
     "exception": false,
     "start_time": "2024-02-21T11:24:26.775372",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, DepthwiseConv2D, SeparableConv2D, MaxPooling2D, Flatten, Dense, Activation\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "\n",
    "# Define the VGG16 architecture with Depthwise Separable Convolution and GELU activation\n",
    "def DepthwiseSeparable_gelu2(input_layer):\n",
    "    # input_layer = Input(shape=input_shape)\n",
    "\n",
    "    # Block 1\n",
    "#     x = DepthwiseConv2D(3, padding='same')(input_layer)\n",
    "#     x = SeparableConv2D(64, (3, 3), padding='same')(x)\n",
    "#     x = Activation(gelu)(x)\n",
    "# #     #x = MaxPooling2D((2, 2), strides=(2, 2))(x)\n",
    "\n",
    "# #     # Block 3\n",
    "#     x = DepthwiseConv2D(3, padding='same')(x)\n",
    "#     x = SeparableConv2D(256, (3, 3), padding='same')(x)\n",
    "#     x = Activation(gelu)(x)\n",
    "#     x = SeparableConv2D(256, (3, 3), padding='same')(x)\n",
    "#     x = Activation(gelu)(x)\n",
    "# #     #x = MaxPooling2D((2, 2), strides=(2, 2))(x)\n",
    "\n",
    "#     # Block 4\n",
    "#     x = DepthwiseConv2D(3, padding='same')(x)\n",
    "#     x = SeparableConv2D(512, (3, 3), padding='same')(x)\n",
    "#     x = Activation(gelu)(x)\n",
    "#     x = SeparableConv2D(512, (3, 3), padding='same')(x)\n",
    "#     x = Activation(gelu)(x)\n",
    "    #x = MaxPooling2D((2, 2), strides=(2, 2))(x)\n",
    "\n",
    "    # Block 6\n",
    "    x = DepthwiseConv2D(3, padding='same')(input_layer)\n",
    "    x = SeparableConv2D(728, (3, 3), padding='same')(x)\n",
    "    x = Activation(gelu)(x)\n",
    "    x = SeparableConv2D(728, (3, 3), padding='same')(x)\n",
    "    x = Activation(gelu)(x)\n",
    "    #x = MaxPooling2D((2, 2), strides=(2, 2))(x)\n",
    "\n",
    "\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6c23601",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T11:24:26.833447Z",
     "iopub.status.busy": "2024-02-21T11:24:26.833141Z",
     "iopub.status.idle": "2024-02-21T11:24:26.840139Z",
     "shell.execute_reply": "2024-02-21T11:24:26.839243Z"
    },
    "id": "VLkyddq4t6nK",
    "papermill": {
     "duration": 0.023471,
     "end_time": "2024-02-21T11:24:26.842033",
     "exception": false,
     "start_time": "2024-02-21T11:24:26.818562",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, DepthwiseConv2D, SeparableConv2D, MaxPooling2D, Flatten, Dense, Activation\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "\n",
    "# Define the VGG16 architecture with Depthwise Separable Convolution and GELU activation\n",
    "def DepthwiseSeparable_gelu3(input_layer):\n",
    "    # input_layer = Input(shape=input_shape)\n",
    "\n",
    "    # Block 1\n",
    "#     x = DepthwiseConv2D(3, padding='same')(input_layer)\n",
    "#     x = SeparableConv2D(64, (3, 3), padding='same')(x)\n",
    "#     x = Activation(gelu)(x)\n",
    "#     #x = MaxPooling2D((2, 2), strides=(2, 2))(x)\n",
    "\n",
    "#     # Block 3\n",
    "#     x = DepthwiseConv2D(3, padding='same')(x)\n",
    "#     x = SeparableConv2D(256, (3, 3), padding='same')(x)\n",
    "#     x = Activation(gelu)(x)\n",
    "#     x = SeparableConv2D(256, (3, 3), padding='same')(x)\n",
    "#     x = Activation(gelu)(x)\n",
    "#     #x = MaxPooling2D((2, 2), strides=(2, 2))(x)\n",
    "\n",
    "#     #Block 5\n",
    "#     x = DepthwiseConv2D(3, padding='same')(x)\n",
    "#     x = SeparableConv2D(512, (3, 3), padding='same')(x)\n",
    "#     x = Activation(gelu)(x)\n",
    "#     x = SeparableConv2D(512, (3, 3), padding='same')(x)\n",
    "#     x = Activation(gelu)(x)\n",
    "    #x = MaxPooling2D((2, 2), strides=(2, 2))(x)\n",
    "\n",
    "    # Block 7\n",
    "    x = DepthwiseConv2D(3, padding='same')(input_layer)\n",
    "    x = SeparableConv2D(512, (3, 3), padding='same')(x)\n",
    "    x = Activation(gelu)(x)\n",
    "    x = SeparableConv2D(1024, 1, padding='same')(x)\n",
    "    x = Activation(gelu)(x)\n",
    "    #x = MaxPooling2D((2, 2), strides=(2, 2))(x)\n",
    "\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f58bebd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T11:24:26.870623Z",
     "iopub.status.busy": "2024-02-21T11:24:26.870365Z",
     "iopub.status.idle": "2024-02-21T11:24:26.878699Z",
     "shell.execute_reply": "2024-02-21T11:24:26.877893Z"
    },
    "id": "TVlqOxGSIRh-",
    "papermill": {
     "duration": 0.024647,
     "end_time": "2024-02-21T11:24:26.880566",
     "exception": false,
     "start_time": "2024-02-21T11:24:26.855919",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "####  Transition Layer  ######\n",
    "def transition_block(inputs):\n",
    "    x = tf.keras.layers.SeparableConv2D(filters=64, kernel_size=1)(inputs)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.ReLU()(x)\n",
    "    x = tf.keras.layers.SeparableConv2D(filters=128, kernel_size=3, padding='same')(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.ReLU()(x)\n",
    "    x = tf.keras.layers.SeparableConv2D(filters=256, kernel_size=1)(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.ReLU()(x)\n",
    "    #x = tf.keras.layers.UpSampling2D((2,2))(x)\n",
    "    x = tf.keras.layers.Conv2DTranspose(filters=256, kernel_size=(3, 3), strides=(2, 2), padding='same')(x)\n",
    "    x = tf.keras.layers.MaxPooling2D((2,2))(x)\n",
    "\n",
    "\n",
    "\n",
    "    return x\n",
    "\n",
    "def additionalL(inputs):\n",
    "    x = tf.keras.layers.SeparableConv2D(filters=256, kernel_size=1)(inputs)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4bc2d294",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T11:24:26.909356Z",
     "iopub.status.busy": "2024-02-21T11:24:26.909044Z",
     "iopub.status.idle": "2024-02-21T11:24:26.930699Z",
     "shell.execute_reply": "2024-02-21T11:24:26.929766Z"
    },
    "id": "7K-LC9A6t30U",
    "papermill": {
     "duration": 0.038505,
     "end_time": "2024-02-21T11:24:26.932644",
     "exception": false,
     "start_time": "2024-02-21T11:24:26.894139",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "class MBConv(tf.keras.layers.Layer):\n",
    "    def __init__(self, filters, kernel_size, strides = 1, expand_ratio = 1, se_ratio = 4, residual = True, momentum = 0.9, epsilon = 0.01, convolution = tf.keras.layers.Conv2D, activation = tf.nn.swish, kernel_initializer = \"he_normal\", **kwargs):\n",
    "        super(MBConv, self).__init__(**kwargs)\n",
    "        self.filters = filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.strides = strides\n",
    "        self.expand_ratio = expand_ratio\n",
    "        self.se_ratio = se_ratio\n",
    "        self.residual = residual\n",
    "        self.momentum = momentum\n",
    "        self.epsilon = epsilon\n",
    "        self.convolution = convolution\n",
    "        self.activation = activation\n",
    "        self.kernel_initializer = kernel_initializer\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.layers = []\n",
    "        self.post = []\n",
    "        if self.expand_ratio != 1:\n",
    "            conv = self.convolution(input_shape[-1] * self.expand_ratio, 1, use_bias = False, kernel_initializer = self.kernel_initializer)\n",
    "            norm = tf.keras.layers.BatchNormalization(momentum = self.momentum, epsilon = self.epsilon)\n",
    "            act = tf.keras.layers.Activation(self.activation)\n",
    "            input_shape = input_shape[:-1] + (input_shape[-1] * self.expand_ratio,)\n",
    "            self.layers += [conv, norm, act]\n",
    "\n",
    "        #Depthwise Convolution\n",
    "        conv = self.convolution(input_shape[-1], self.kernel_size, strides = self.strides, groups = input_shape[-1], padding = \"same\", use_bias = False, kernel_initializer = self.kernel_initializer)\n",
    "        norm = tf.keras.layers.BatchNormalization(momentum = self.momentum, epsilon = self.epsilon)\n",
    "        act = tf.keras.layers.Activation(self.activation)\n",
    "        self.layers += [conv, norm, act]\n",
    "\n",
    "        #Squeeze and Excitation layer, if desired\n",
    "        axis = list(range(1, len(input_shape) - 1))\n",
    "        gap = tf.keras.layers.Lambda(lambda x: tf.reduce_mean(x, axis = axis, keepdims = True))\n",
    "        squeeze = self.convolution(max(1, int(input_shape[-1] / self.se_ratio)), 1, use_bias = True, kernel_initializer = self.kernel_initializer)\n",
    "        act = tf.keras.layers.Activation(self.activation)\n",
    "        excitation = self.convolution(input_shape[-1], 1, use_bias = True, kernel_initializer = self.kernel_initializer)\n",
    "        se = lambda x: x * tf.nn.sigmoid(excitation(act(squeeze(gap(x)))))\n",
    "        self.layers += [se]\n",
    "\n",
    "        #Output Phase\n",
    "        conv = self.convolution(self.filters, 1, use_bias = False, kernel_initializer = self.kernel_initializer)\n",
    "        norm = tf.keras.layers.BatchNormalization(momentum = self.momentum, epsilon = self.epsilon)\n",
    "        self.layers += [conv, norm]\n",
    "\n",
    "        #Residual\n",
    "        if self.residual:\n",
    "            if 1 < self.strides:\n",
    "                pool = tf.keras.layers.MaxPool2D(pool_size = self.strides + 1, strides = self.strides, padding = \"same\")\n",
    "                self.post.append(pool)\n",
    "            if input_shape[-1] != self.filters:\n",
    "                resample = self.convolution(self.filters, 1, use_bias = False, kernel_initializer = self.kernel_initializer)\n",
    "                self.post.append(resample)\n",
    "\n",
    "    def call(self, x):\n",
    "        out = x\n",
    "        for layer in self.layers:\n",
    "            out = layer(out)\n",
    "\n",
    "        if self.residual:\n",
    "            for layer in self.post:\n",
    "                x = layer(x)\n",
    "            out = out + x\n",
    "        return out\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(MBConv, self).get_config()\n",
    "        config[\"filters\"] = self.filters\n",
    "        config[\"kernel_size\"] = self.kernel_size\n",
    "        config[\"expand_ratio\"] = self.expand_ratio\n",
    "        config[\"se_ratio\"] = self.se_ratio\n",
    "        config[\"residual\"] = self.residual\n",
    "        config[\"momentum\"] = self.momentum\n",
    "        config[\"epsilon\"] = self.epsilon\n",
    "        return config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4da57508",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T11:24:26.962909Z",
     "iopub.status.busy": "2024-02-21T11:24:26.962626Z",
     "iopub.status.idle": "2024-02-21T11:24:26.987284Z",
     "shell.execute_reply": "2024-02-21T11:24:26.986576Z"
    },
    "id": "5ccuaututz3s",
    "papermill": {
     "duration": 0.042199,
     "end_time": "2024-02-21T11:24:26.989321",
     "exception": false,
     "start_time": "2024-02-21T11:24:26.947122",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, emb_dim = 768, n_head = 12, out_dim = None, relative_window_size = None, dropout_rate = 0., kernel_initializer = tf.keras.initializers.RandomNormal(mean = 0, stddev = 0.01), **kwargs):\n",
    "        #ScaledDotProductAttention\n",
    "        super(MultiHeadSelfAttention, self).__init__(**kwargs)\n",
    "        self.emb_dim = emb_dim\n",
    "        self.n_head = n_head\n",
    "        if emb_dim % n_head != 0:\n",
    "            raise ValueError(\"Shoud be embedding dimension % number of heads = 0.\")\n",
    "        if out_dim is None:\n",
    "            out_dim = self.emb_dim\n",
    "        self.out_dim = out_dim\n",
    "        if relative_window_size is not None and np.ndim(relative_window_size) == 0:\n",
    "            relative_window_size = [relative_window_size, relative_window_size]\n",
    "        self.relative_window_size = relative_window_size\n",
    "        self.projection_dim = emb_dim // n_head\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.query = tf.keras.layers.Dense(emb_dim, kernel_initializer = kernel_initializer)\n",
    "        self.key = tf.keras.layers.Dense(emb_dim, kernel_initializer = kernel_initializer)\n",
    "        self.value = tf.keras.layers.Dense(emb_dim, kernel_initializer = kernel_initializer)\n",
    "        self.combine = tf.keras.layers.Dense(out_dim, kernel_initializer = kernel_initializer)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        if self.relative_window_size is not None:\n",
    "            self.relative_position_bias_table = self.add_weight(\"relative_position_bias_table\", shape = [((2 * self.relative_window_size[0]) - 1) * ((2 * self.relative_window_size[1]) - 1), self.n_head], trainable = self.trainable)\n",
    "            coords_h = np.arange(self.relative_window_size[0])\n",
    "            coords_w = np.arange(self.relative_window_size[1])\n",
    "            coords = np.stack(np.meshgrid(coords_h, coords_w, indexing = \"ij\")) #2, Wh, Ww\n",
    "            coords = np.reshape(coords, [2, -1])\n",
    "            relative_coords = np.expand_dims(coords, axis = -1) - np.expand_dims(coords, axis = -2) #2, Wh * Ww, Wh * Ww\n",
    "            relative_coords = np.transpose(relative_coords, [1, 2, 0]) #Wh * Ww, Wh * Ww, 2\n",
    "            relative_coords[:, :, 0] += self.relative_window_size[0] - 1 #shift to start from 0\n",
    "            relative_coords[:, :, 1] += self.relative_window_size[1] - 1\n",
    "            relative_coords[:, :, 0] *= 2 * self.relative_window_size[1] - 1\n",
    "            relative_position_index = np.sum(relative_coords, -1)\n",
    "            self.relative_position_index = tf.Variable(tf.convert_to_tensor(relative_position_index), trainable = False, name= \"relative_position_index\")\n",
    "\n",
    "    def attention(self, query, key, value, relative_position_bias = None):\n",
    "        score = tf.matmul(query, key, transpose_b = True)\n",
    "        n_key = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "        scaled_score = score / tf.math.sqrt(n_key)\n",
    "        if relative_position_bias is not None:\n",
    "            scaled_score = scaled_score + relative_position_bias\n",
    "        weight = tf.nn.softmax(scaled_score, axis = -1)\n",
    "        if 0 < self.dropout_rate:\n",
    "            weight = tf.nn.dropout(weight, self.dropout_rate)\n",
    "        out = tf.matmul(weight, value)\n",
    "        return out\n",
    "\n",
    "    def separate_head(self, x):\n",
    "        out = tf.keras.layers.Reshape([-1, self.n_head, self.projection_dim])(x)\n",
    "        out = tf.keras.layers.Permute([2, 1, 3])(out)\n",
    "        return out\n",
    "\n",
    "    def call(self, inputs):\n",
    "        query = self.query(inputs)\n",
    "        key = self.key(inputs)\n",
    "        value = self.value(inputs)\n",
    "\n",
    "        query = self.separate_head(query)\n",
    "        key = self.separate_head(key)\n",
    "        value = self.separate_head(value)\n",
    "\n",
    "        relative_position_bias = None\n",
    "        if self.relative_window_size is not None:\n",
    "            relative_position_bias = tf.gather(self.relative_position_bias_table, tf.reshape(self.relative_position_index, [-1]))\n",
    "            relative_position_bias = tf.reshape(relative_position_bias, [self.relative_window_size[0] * self.relative_window_size[1], self.relative_window_size[0] * self.relative_window_size[1], -1]) #Wh * Ww,Wh * Ww, nH\n",
    "            relative_position_bias = tf.transpose(relative_position_bias, [2, 0, 1]) #nH, Wh * Ww, Wh * Ww\n",
    "            relative_position_bias = tf.expand_dims(relative_position_bias, axis = 0)\n",
    "        attention = self.attention(query, key, value, relative_position_bias)\n",
    "        attention = tf.keras.layers.Permute([2, 1, 3])(attention)\n",
    "        attention = tf.keras.layers.Reshape([-1, self.emb_dim])(attention)\n",
    "\n",
    "        out = self.combine(attention)\n",
    "        return out\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(MultiHeadSelfAttention, self).get_config()\n",
    "        config[\"emb_dim\"] = self.emb_dim\n",
    "        config[\"n_head\"] = self.n_head\n",
    "        config[\"out_dim\"] = self.out_dim\n",
    "        config[\"relative_window_size\"] = self.relative_window_size\n",
    "        config[\"projection_dim\"] = self.projection_dim\n",
    "        config[\"dropout_rate\"] = self.dropout_rate\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "676bf4b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T11:24:27.019138Z",
     "iopub.status.busy": "2024-02-21T11:24:27.018899Z",
     "iopub.status.idle": "2024-02-21T11:24:27.034277Z",
     "shell.execute_reply": "2024-02-21T11:24:27.033402Z"
    },
    "id": "oulnq-Jgpu5u",
    "papermill": {
     "duration": 0.03323,
     "end_time": "2024-02-21T11:24:27.036387",
     "exception": false,
     "start_time": "2024-02-21T11:24:27.003157",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ConvTransformer(tf.keras.layers.Layer):\n",
    "    def __init__(self, emb_dim = 768, n_head = 12, strides = 1, out_dim = None, epsilon = 1e-5, dropout_rate = 0., activation = tf.keras.activations.gelu, kernel_initializer = tf.keras.initializers.RandomNormal(mean = 0, stddev = 0.01), **kwargs):\n",
    "        super(ConvTransformer, self).__init__(**kwargs)\n",
    "        self.emb_dim = emb_dim\n",
    "        self.n_head = n_head\n",
    "        self.strides = strides\n",
    "        self.out_dim = out_dim if out_dim is not None else emb_dim\n",
    "        self.epsilon = epsilon\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.activation = activation\n",
    "        self.kernel_initializer = kernel_initializer\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.attention = []\n",
    "        self.residual = []\n",
    "\n",
    "        #Attention\n",
    "        shape = input_shape[1:3]\n",
    "        if 1 < self.strides:\n",
    "            shape = np.divide(np.add(shape, (self.strides - 1)), self.strides).astype(int)\n",
    "            pool = tf.keras.layers.MaxPool2D(pool_size = self.strides + 1, strides = self.strides, padding = \"same\")\n",
    "            self.attention.append(pool)\n",
    "            self.residual.append(pool)\n",
    "        if input_shape[-1] != self.out_dim:\n",
    "            resample = tf.keras.layers.Conv2D(self.out_dim, 1, padding = \"same\", use_bias = False, kernel_initializer = \"he_normal\")\n",
    "            self.residual.append(resample)\n",
    "        pre_reshape = tf.keras.layers.Reshape([-1, input_shape[-1]])\n",
    "        mhsa = MultiHeadSelfAttention(emb_dim = self.emb_dim, n_head = self.n_head, out_dim = self.out_dim, relative_window_size = shape, dropout_rate = self.dropout_rate)\n",
    "        post_reshape = tf.keras.layers.Reshape([*shape, self.out_dim])\n",
    "        self.attention += [pre_reshape, mhsa, post_reshape]\n",
    "\n",
    "        self.ffn = []\n",
    "        #Feed Forward Network\n",
    "        norm = tf.keras.layers.LayerNormalization(epsilon = self.epsilon)\n",
    "        dense1 = tf.keras.layers.Dense(self.out_dim, kernel_initializer = self.kernel_initializer)\n",
    "        act = tf.keras.layers.Activation(self.activation)\n",
    "        dense2 = tf.keras.layers.Dense(self.out_dim, kernel_initializer = self.kernel_initializer)\n",
    "        self.ffn = [norm, dense1, act, dense2]\n",
    "\n",
    "    def call(self, inputs):\n",
    "        out = inputs\n",
    "        for layer in self.attention:\n",
    "            out = layer(out)\n",
    "        for layer in self.residual:\n",
    "            inputs = layer(inputs)\n",
    "        out = out + inputs\n",
    "\n",
    "        for layer in self.ffn:\n",
    "            out = layer(out)\n",
    "        return out\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(ConvTransformer, self).get_config()\n",
    "        config[\"emb_dim\"] = self.emb_dim\n",
    "        config[\"n_head\"] = self.n_head\n",
    "        config[\"strides\"] = self.strides\n",
    "        config[\"out_dim\"] = self.out_dim\n",
    "        config[\"epsilon\"] = self.epsilon\n",
    "        config[\"dropout_rate\"] = self.dropout_rate\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c81279f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T11:24:27.065673Z",
     "iopub.status.busy": "2024-02-21T11:24:27.065426Z",
     "iopub.status.idle": "2024-02-21T11:24:27.071807Z",
     "shell.execute_reply": "2024-02-21T11:24:27.070974Z"
    },
    "id": "wRPPqtoBISew",
    "papermill": {
     "duration": 0.022697,
     "end_time": "2024-02-21T11:24:27.073725",
     "exception": false,
     "start_time": "2024-02-21T11:24:27.051028",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def entry_flow(inputs) :\n",
    "\n",
    "    x = inputs\n",
    "    previous_block_activation = x\n",
    "\n",
    "    for size in [256, 728] :\n",
    "\n",
    "        x = Activation('relu')(x)\n",
    "        x = SeparableConv2D(size, 3, padding='same')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "\n",
    "        x = Activation('relu')(x)\n",
    "        x = SeparableConv2D(size, 3, padding='same')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "\n",
    "        x = MaxPooling2D(3, strides=2, padding='same')(x)\n",
    "\n",
    "        residual = Conv2D(size, 1, strides=2, padding='same')(previous_block_activation)\n",
    "\n",
    "        x = tensorflow.keras.layers.Add()([x, residual])\n",
    "        previous_block_activation = x\n",
    "\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "16420e7b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T11:24:27.102183Z",
     "iopub.status.busy": "2024-02-21T11:24:27.101943Z",
     "iopub.status.idle": "2024-02-21T11:24:27.108208Z",
     "shell.execute_reply": "2024-02-21T11:24:27.107385Z"
    },
    "id": "Fyh27-qnIWCJ",
    "papermill": {
     "duration": 0.022622,
     "end_time": "2024-02-21T11:24:27.110049",
     "exception": false,
     "start_time": "2024-02-21T11:24:27.087427",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def middle_flow(x, num_blocks=8) :\n",
    "\n",
    "    previous_block_activation = x\n",
    "\n",
    "    for _ in range(num_blocks) :\n",
    "\n",
    "        x = Activation('relu')(x)\n",
    "        x = SeparableConv2D(728, 3, padding='same')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "\n",
    "        x = Activation('relu')(x)\n",
    "        x = SeparableConv2D(728, 3, padding='same')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "\n",
    "        x = Activation('relu')(x)\n",
    "        x = SeparableConv2D(728, 3, padding='same')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "\n",
    "        x = tensorflow.keras.layers.Add()([x, previous_block_activation])\n",
    "        previous_block_activation = x\n",
    "\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8485b52b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T11:24:27.139005Z",
     "iopub.status.busy": "2024-02-21T11:24:27.138765Z",
     "iopub.status.idle": "2024-02-21T11:24:27.146058Z",
     "shell.execute_reply": "2024-02-21T11:24:27.145247Z"
    },
    "id": "KHyqt54VIV_m",
    "papermill": {
     "duration": 0.023799,
     "end_time": "2024-02-21T11:24:27.147855",
     "exception": false,
     "start_time": "2024-02-21T11:24:27.124056",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def exit_flow(x) :\n",
    "\n",
    "    previous_block_activation = x\n",
    "\n",
    "    x = Activation('relu')(x)\n",
    "    x = SeparableConv2D(728, 3, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = Activation('relu')(x)\n",
    "    x = SeparableConv2D(1024, 3, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = MaxPooling2D(3, strides=2, padding='same')(x)\n",
    "\n",
    "    residual = Conv2D(1024, 1, strides=2, padding='same')(previous_block_activation)\n",
    "    x = tensorflow.keras.layers.Add()([x, residual])\n",
    "\n",
    "    x = Activation('relu')(x)\n",
    "    x = SeparableConv2D(1536, 3, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = Activation('relu')(x)\n",
    "    x = SeparableConv2D(2048, 3, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f5495d7b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T11:24:27.176272Z",
     "iopub.status.busy": "2024-02-21T11:24:27.175990Z",
     "iopub.status.idle": "2024-02-21T11:24:27.187763Z",
     "shell.execute_reply": "2024-02-21T11:24:27.187065Z"
    },
    "id": "yHGaX_35PgOt",
    "papermill": {
     "duration": 0.027987,
     "end_time": "2024-02-21T11:24:27.189571",
     "exception": false,
     "start_time": "2024-02-21T11:24:27.161584",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def identity_block(input_tensor, filters, stage, block):\n",
    "    filters1, filters2, filters3 = filters\n",
    "\n",
    "    conv_name_base = f'resid_{stage}_branch{block}_'\n",
    "    bn_name_base = f'bn_{stage}_branch{block}_'\n",
    "\n",
    "    x = Conv2D(filters1, (1, 1), name=conv_name_base + '2a')(input_tensor)\n",
    "    x = BatchNormalization(name=bn_name_base + '2a')(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Conv2D(filters2, kernel_size=(3, 3), padding='same', name=conv_name_base + '2b')(x)\n",
    "    x = BatchNormalization(name=bn_name_base + '2b')(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Conv2D(filters3, (1, 1), name=conv_name_base + '2c')(x)\n",
    "    x = BatchNormalization(name=bn_name_base + '2c')(x)\n",
    "\n",
    "    x = Add()([x, input_tensor])\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "def conv_block(input_tensor, filters, stage, block, strides=(2, 2)):\n",
    "    filters1, filters2, filters3 = filters\n",
    "\n",
    "    conv_name_base = f'resid_{stage}_branch'\n",
    "    bn_name_base = f'bn_{stage}_branch'\n",
    "\n",
    "    x = Conv2D(filters1, (1, 1), strides=strides, name=conv_name_base + '2a')(input_tensor)\n",
    "    x = BatchNormalization(name=bn_name_base + '2a')(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Conv2D(filters2, kernel_size=(3, 3), padding='same', name=conv_name_base + '2b')(x)\n",
    "    x = BatchNormalization(name=bn_name_base + '2b')(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Conv2D(filters3, (1, 1), name=conv_name_base + '2c')(x)\n",
    "    x = BatchNormalization(name=bn_name_base + '2c')(x)\n",
    "\n",
    "    shortcut = Conv2D(filters3, (1, 1), strides=strides, name=conv_name_base + '1')(input_tensor)\n",
    "    shortcut = BatchNormalization(name=bn_name_base + '1')(shortcut)\n",
    "\n",
    "    x = Add()([x, shortcut])\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "05b3c04d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T11:24:27.219148Z",
     "iopub.status.busy": "2024-02-21T11:24:27.218486Z",
     "iopub.status.idle": "2024-02-21T11:24:27.227265Z",
     "shell.execute_reply": "2024-02-21T11:24:27.226419Z"
    },
    "id": "pSUqoanggr6C",
    "papermill": {
     "duration": 0.025487,
     "end_time": "2024-02-21T11:24:27.229098",
     "exception": false,
     "start_time": "2024-02-21T11:24:27.203611",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def Transformer(input):\n",
    "    out = tf.keras.layers.Conv2D(1024, 3, strides = 1, padding = \"same\", use_bias = False, kernel_initializer = \"he_normal\")(input)\n",
    "    out = tf.keras.layers.BatchNormalization(momentum = 0.9, epsilon = 1e-5)(out)\n",
    "    out = tf.keras.layers.Activation(gelu)(out)\n",
    "    ## M ##\n",
    "    out = tf.keras.layers.BatchNormalization(momentum = 0.9, epsilon = 1e-5)(out)\n",
    "    out = MBConv(512, 3, strides = 1, expand_ratio = 4, se_ratio = 4, residual = True, momentum = 0.9, epsilon = 1e-5, activation = gelu)(out)\n",
    "\n",
    "    out = tf.keras.layers.BatchNormalization(momentum = 0.9, epsilon = 1e-5)(out)\n",
    "    out = MBConv(512, 3, strides = 1, expand_ratio = 4, se_ratio = 4, residual = True, momentum = 0.9, epsilon = 1e-5, activation = gelu)(out)\n",
    "    ## T ##\n",
    "    out = tf.keras.layers.LayerNormalization(epsilon = 1e-5)(out)\n",
    "    out = ConvTransformer(16 * 8, 8, strides = 1, out_dim = 512, epsilon = 1e-5, activation = gelu)(out)\n",
    "\n",
    "    out = tf.keras.layers.LayerNormalization(epsilon = 1e-5)(out)\n",
    "    trans_out1 = ConvTransformer(16 * 8, 8, strides = 1, out_dim = 512, epsilon = 1e-5, activation = gelu)(out)\n",
    "    return trans_out1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e1808ef5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T11:24:27.257711Z",
     "iopub.status.busy": "2024-02-21T11:24:27.257457Z",
     "iopub.status.idle": "2024-02-21T11:24:40.472786Z",
     "shell.execute_reply": "2024-02-21T11:24:40.471491Z"
    },
    "papermill": {
     "duration": 13.232426,
     "end_time": "2024-02-21T11:24:40.475155",
     "exception": false,
     "start_time": "2024-02-21T11:24:27.242729",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow-addons\r\n",
      "  Downloading tensorflow_addons-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.8 kB)\r\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorflow-addons) (21.3)\r\n",
      "Collecting typeguard<3.0.0,>=2.7 (from tensorflow-addons)\r\n",
      "  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->tensorflow-addons) (3.1.1)\r\n",
      "Downloading tensorflow_addons-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (611 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m611.8/611.8 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: typeguard, tensorflow-addons\r\n",
      "  Attempting uninstall: typeguard\r\n",
      "    Found existing installation: typeguard 4.1.5\r\n",
      "    Uninstalling typeguard-4.1.5:\r\n",
      "      Successfully uninstalled typeguard-4.1.5\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "ydata-profiling 4.6.4 requires typeguard<5,>=4.1.2, but you have typeguard 2.13.3 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed tensorflow-addons-0.23.0 typeguard-2.13.3\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow-addons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "163f7b18",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T11:24:40.507329Z",
     "iopub.status.busy": "2024-02-21T11:24:40.506996Z",
     "iopub.status.idle": "2024-02-21T11:24:53.358726Z",
     "shell.execute_reply": "2024-02-21T11:24:53.357519Z"
    },
    "papermill": {
     "duration": 12.870336,
     "end_time": "2024-02-21T11:24:53.361301",
     "exception": false,
     "start_time": "2024-02-21T11:24:40.490965",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting vit_keras\r\n",
      "  Downloading vit_keras-0.1.2-py3-none-any.whl.metadata (4.0 kB)\r\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from vit_keras) (1.11.4)\r\n",
      "Collecting validators (from vit_keras)\r\n",
      "  Downloading validators-0.22.0-py3-none-any.whl.metadata (4.7 kB)\r\n",
      "Requirement already satisfied: numpy<1.28.0,>=1.21.6 in /opt/conda/lib/python3.10/site-packages (from scipy->vit_keras) (1.24.4)\r\n",
      "Downloading vit_keras-0.1.2-py3-none-any.whl (24 kB)\r\n",
      "Downloading validators-0.22.0-py3-none-any.whl (26 kB)\r\n",
      "Installing collected packages: validators, vit_keras\r\n",
      "Successfully installed validators-0.22.0 vit_keras-0.1.2\r\n"
     ]
    }
   ],
   "source": [
    "!pip install vit_keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "91a4cd5a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T11:24:53.396540Z",
     "iopub.status.busy": "2024-02-21T11:24:53.395744Z",
     "iopub.status.idle": "2024-02-21T11:24:53.401445Z",
     "shell.execute_reply": "2024-02-21T11:24:53.400537Z"
    },
    "papermill": {
     "duration": 0.025521,
     "end_time": "2024-02-21T11:24:53.403816",
     "exception": false,
     "start_time": "2024-02-21T11:24:53.378295",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def VM(input_size = (128,128,3),input = input):\n",
    "    vision_model = vit.vit_b16(input_size[0])(input)\n",
    "    M= tf.keras.Model(inputs=input, outputs=vision_model)\n",
    "    return M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6e6c35c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T11:24:53.436069Z",
     "iopub.status.busy": "2024-02-21T11:24:53.435820Z",
     "iopub.status.idle": "2024-02-21T11:24:53.480325Z",
     "shell.execute_reply": "2024-02-21T11:24:53.479616Z"
    },
    "id": "xgtrlHDaIV8f",
    "papermill": {
     "duration": 0.062713,
     "end_time": "2024-02-21T11:24:53.482095",
     "exception": false,
     "start_time": "2024-02-21T11:24:53.419382",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import VGG16, ResNet50, VGG19,ResNet101\n",
    "import tensorflow\n",
    "def make_model(input_size = (128,128,3)):\n",
    "    input = tf.keras.layers.Input(input_size)\n",
    "\n",
    "\n",
    "     # Define XceptionNet model\n",
    "    base_model1 = Xception(\n",
    "        include_top = False,\n",
    "        weights     = 'imagenet',\n",
    "        pooling     = 'max',\n",
    "        input_shape=(128, 128, 3),\n",
    "        input_tensor=input\n",
    "    )\n",
    "\n",
    "    # Define ResNet50 model\n",
    "    base_model2 = ResNet50(\n",
    "        include_top = False,\n",
    "        weights     = 'imagenet',\n",
    "        pooling     = 'max',\n",
    "        input_shape=(128, 128, 3),\n",
    "        input_tensor=input\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    for layer in base_model1.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    # Set Xception layers to non-trainable\n",
    "    for layer in base_model2.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "\n",
    "\n",
    "    #Getting outputs from multiple layers of XceptionNet\n",
    "    last_layer11 = base_model1.get_layer('block3_sepconv1').output\n",
    "    last_layer12 = base_model1.get_layer('block5_sepconv1').output\n",
    "    #last_layer13 = base_model1.get_layer('block10_sepconv1_act').output\n",
    "    last_layer13 = base_model1.get_layer('block13_sepconv2').output\n",
    "    last_layer14 = base_model1.get_layer('block14_sepconv2_act').output\n",
    "\n",
    "\n",
    "\n",
    "    #Getting outputs from multiple layers of ResNet50\n",
    "    last_layer21 = base_model2.get_layer('conv2_block3_out').output\n",
    "    last_layer22 = base_model2.get_layer('conv3_block4_out').output\n",
    "    #last_layer23 = base_model2.get_layer('conv4_block4_out').output\n",
    "    last_layer23 = base_model2.get_layer('conv4_block6_out').output\n",
    "    last_layer24 = base_model2.get_layer('conv5_block3_out').output\n",
    "\n",
    "\n",
    "    ############ Layers for Xception ##############@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
    "    ########### first feature share ###########\n",
    "    layerX1 = base_model1.get_layer('block3_sepconv2_act').output\n",
    "    layerX1 = DepthwiseSeparable_gelu0(layerX1)\n",
    "\n",
    "    ## x, n_class = 1000, include_top = True, n_depth = [2, 2, 6, 14, 2], n_feature = [64, 96, 192, 384, 768], block = [\"C\", \"M\", \"M\", \"T\", \"T\"], stage_stride_size = 2, expand_ratio = 4, se_ratio = 4, dropout_rate = 0., activation = tf.keras.activations.gelu, name = \"\"\n",
    "    layer16 = tf.keras.layers.BatchNormalization(momentum = 0.9, epsilon = 1e-5)(last_layer21)\n",
    "    #layer16 = MBConv(64, 3, strides = 1, expand_ratio = 4, se_ratio = 4, residual = True, momentum = 0.9, epsilon = 1e-5, activation = tf.keras.activations.gelu)(layer16)\n",
    "    #layer16 = MBConv(128, 3, strides = 1, expand_ratio = 4, se_ratio = 4, residual = True, momentum = 0.9, epsilon = 1e-5, activation = tf.keras.activations.gelu)(layer16)\n",
    "    layer16 = MBConv(256, 3, strides = 1, expand_ratio = 4, se_ratio = 4, residual = True, momentum = 0.9, epsilon = 1e-5, activation = tf.keras.activations.gelu)(layer16)\n",
    "    layer16 = MaxPooling2D((2, 2), strides=1)(layer16)\n",
    "\n",
    "\n",
    "    layerX1 = tf.keras.layers.concatenate([layer16,layerX1])\n",
    "    layerX2 = entry_flow(layerX1)\n",
    "\n",
    "\n",
    "    ############ second feature share #########\n",
    "\n",
    "    layerX2 = DepthwiseSeparable_gelu2(layerX2)\n",
    "    #print(layerX2.shape)\n",
    "\n",
    "    layer17 = tf.keras.layers.BatchNormalization(momentum = 0.9, epsilon = 1e-5)(last_layer22)\n",
    "    #layer17 = MBConv(256, 3, strides = 1, expand_ratio = 4, se_ratio = 4, residual = True, momentum = 0.9, epsilon = 1e-5, activation = tf.keras.activations.gelu)(layer17)\n",
    "    #layer17 = MBConv(512, 3, strides = 1, expand_ratio = 4, se_ratio = 4, residual = True, momentum = 0.9, epsilon = 1e-5, activation = tf.keras.activations.gelu)(layer17)\n",
    "    layer17 = MBConv(728, 3, strides = 2, expand_ratio = 4, se_ratio = 4, residual = True, momentum = 0.9, epsilon = 1e-5, activation = tf.keras.activations.gelu)(layer17)\n",
    "    #print(layer17.shape)\n",
    "\n",
    "    layerX2 = tf.keras.layers.concatenate([layerX2,layer17])\n",
    "    layerX2 = Conv2D(filters=728, kernel_size=1, strides=1, padding='same')(layerX2)\n",
    "    #print(layerX2.shape)\n",
    "    layerX3 = middle_flow(layerX2)\n",
    "    #print(layerX3.shape)\n",
    "\n",
    "    ############ third feature share ##########\n",
    "\n",
    "    layerX3 = DepthwiseSeparable_gelu2(layerX3)\n",
    "    #print(layerX3.shape)\n",
    "\n",
    "    layer18 = tf.keras.layers.BatchNormalization(momentum = 0.9, epsilon = 1e-5)(last_layer23)\n",
    "    #layer18 = MBConv(256, 3, strides = 1, expand_ratio = 4, se_ratio = 4, residual = True, momentum = 0.9, epsilon = 1e-5, activation = tf.keras.activations.gelu)(layer18)\n",
    "    #layer18 = MBConv(512, 3, strides = 1, expand_ratio = 4, se_ratio = 4, residual = True, momentum = 0.9, epsilon = 1e-5, activation = tf.keras.activations.gelu)(layer18)\n",
    "    layer18 = MBConv(728, 3, strides = 1, expand_ratio = 4, se_ratio = 4, residual = True, momentum = 0.9, epsilon = 1e-5, activation = tf.keras.activations.gelu)(layer18)\n",
    "    #print(layer18.shape)\n",
    "\n",
    "    layerX3 = tf.keras.layers.concatenate([layerX3,layer18])\n",
    "    layerX3 = Conv2D(filters=728, kernel_size=1, strides=1, padding='same')(layerX3)\n",
    "    #print(layerX3.shape)\n",
    "    feature1 = exit_flow(layerX3)\n",
    "    #print(feature1.shape)\n",
    "\n",
    "\n",
    "    ################# Xception Transformer #####################\n",
    "    trans_out1 = Transformer(feature1)\n",
    "\n",
    "\n",
    "    ############ Layers for ResNet50 ##############@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
    "    ############ first layer ###################\n",
    "    layer150 = base_model2.get_layer('conv2_block3_add').output\n",
    "    layer150 = MaxPooling2D((2, 2), strides=1)(layer150)\n",
    "    layer50 = DepthwiseSeparable_gelu0(layer150)\n",
    "    #print(layer150.shape)\n",
    "    layer19 = tf.keras.layers.BatchNormalization(momentum = 0.9, epsilon = 1e-5)(last_layer11)\n",
    "    #layer19 = MBConv(64, 3, strides = 1, expand_ratio = 4, se_ratio = 4, residual = True, momentum = 0.9, epsilon = 1e-5, activation = tf.keras.activations.gelu)(layer19)\n",
    "    #layer19 = MBConv(128, 3, strides = 1, expand_ratio = 4, se_ratio = 4, residual = True, momentum = 0.9, epsilon = 1e-5, activation = tf.keras.activations.gelu)(layer19)\n",
    "    layer19 = MBConv(256, 3, strides = 1, expand_ratio = 4, se_ratio = 4, residual = True, momentum = 0.9, epsilon = 1e-5, activation = gelu)(layer19)\n",
    "    #print(layer19.shape)\n",
    "    layer150 = tf.keras.layers.concatenate([layer19,layer150])\n",
    "    layer150 = transition_block(layer150)\n",
    "    #print(layer150.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ############ second layer #################\n",
    "    x = Activation('relu')(layer150)\n",
    "    #x = conv_block(x, [64, 64, 256], stage=2, block='a', strides=(1, 1))\n",
    "    x = identity_block(x, [64, 64, 256], stage=2, block='b')\n",
    "    x = identity_block(x, [64, 64, 256], stage=2, block='c')\n",
    "\n",
    "    x = conv_block(x, [128, 128, 512], stage=3, block='a')\n",
    "    x = identity_block(x, [128, 128, 512], stage=3, block='b')\n",
    "    x = identity_block(x, [128, 128, 512], stage=3, block='c')\n",
    "    #x = identity_block(x, [128, 128, 512], stage=3, block='d')\n",
    "    y = x\n",
    "    conv_name_base = f'resid_{3}_branch{\"d\"}_'\n",
    "    bn_name_base = f'bn_{3}_branch{\"d\"}_'\n",
    "\n",
    "    x = Conv2D(128, (1, 1), name=conv_name_base + '2a')(x)\n",
    "    x = BatchNormalization(name=bn_name_base + '2a')(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Conv2D(128, kernel_size=(3, 3), padding='same', name=conv_name_base + '2b')(x)\n",
    "    x = BatchNormalization(name=bn_name_base + '2b')(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Conv2D(512, (1, 1), name=conv_name_base + '2c')(x)\n",
    "    x = BatchNormalization(name=bn_name_base + '2c')(x)\n",
    "\n",
    "    x = Add()([x, y])\n",
    "\n",
    "    x = DepthwiseSeparable_gelu1(x)\n",
    "    #print(x.shape)\n",
    "\n",
    "    #tf.keras.layers.Conv2DTranspose(filters=256, kernel_size=(3, 3), strides=(2, 2), padding='same')\n",
    "    layer29 = tf.keras.layers.Conv2DTranspose(filters=512,kernel_size=(4, 4),strides=2,padding='valid')(last_layer12)\n",
    "    layer29 = tf.keras.layers.Conv2DTranspose(filters=512,kernel_size=(2, 2),strides=2,padding='same')(last_layer12)\n",
    "\n",
    "    layer29 = tf.keras.layers.BatchNormalization(momentum = 0.9, epsilon = 1e-5)(layer29)\n",
    "    #layer29 = MBConv(128, 3, strides = 1, expand_ratio = 4, se_ratio = 4, residual = True, momentum = 0.9, epsilon = 1e-5, activation = tf.keras.activations.gelu)(layer29)\n",
    "    #layer29 = MBConv(256, 3, strides = 1, expand_ratio = 4, se_ratio = 4, residual = True, momentum = 0.9, epsilon = 1e-5, activation = tf.keras.activations.gelu)(layer29)\n",
    "    layer29 = MBConv(512, 3, strides = 1, expand_ratio = 4, se_ratio = 4, residual = True, momentum = 0.9, epsilon = 1e-5, activation = tf.keras.activations.gelu)(layer29)\n",
    "\n",
    "    #print(layer29.shape)\n",
    "    layer250 = tf.keras.layers.concatenate([x,layer29])\n",
    "    layer250 = Conv2D(filters=512, kernel_size=3, strides=1, padding='same')(layer250)\n",
    "    #print(layer250.shape)\n",
    "    # layer250_t = transition_block(layer250)\n",
    "    # layer250_a = transition_block(layer250)\n",
    "    # layer250_ = tf.keras.layers.concatenate([layer250_t,layer250_a])\n",
    "    #print(layer250_.shape)\n",
    "\n",
    "\n",
    "\n",
    "    ############## third layer ###############\n",
    "    x = Activation('relu')(layer250)\n",
    "    x = conv_block(x, [256, 256, 1024], stage=4, block='a')\n",
    "    x = identity_block(x, [256, 256, 1024], stage=4, block='b')\n",
    "    x = identity_block(x, [256, 256, 1024], stage=4, block='c')\n",
    "    x = identity_block(x, [256, 256, 1024], stage=4, block='d')\n",
    "    x = identity_block(x, [256, 256, 1024], stage=4, block='e')\n",
    "    #x = identity_block(x, [256, 256, 1024], stage=4, block='f')\n",
    "\n",
    "    conv_name_base = f'resid_{4}_branch{\"f\"}_'\n",
    "    bn_name_base = f'bn_{4}_branch{\"f\"}_'\n",
    "    y = x\n",
    "    x = Conv2D(256, (1, 1), name=conv_name_base + '2a')(x)\n",
    "    x = BatchNormalization(name=bn_name_base + '2a')(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Conv2D(256, kernel_size=(3, 3), padding='same', name=conv_name_base + '2b')(x)\n",
    "    x = BatchNormalization(name=bn_name_base + '2b')(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Conv2D(1024, (1, 1), name=conv_name_base + '2c')(x)\n",
    "    x = BatchNormalization(name=bn_name_base + '2c')(x)\n",
    "\n",
    "    x = Add()([x, y])\n",
    "    x = DepthwiseSeparable_gelu3(x)\n",
    "    #print(x.shape)\n",
    "\n",
    "    layer99 = tf.keras.layers.BatchNormalization(momentum = 0.9, epsilon = 1e-5)(last_layer13)\n",
    "    #layer99 = MBConv(512, 3, strides = 1, expand_ratio = 4, se_ratio = 4, residual = True, momentum = 0.9, epsilon = 1e-5, activation = tf.keras.activations.gelu)(layer99)\n",
    "    #layer99 = MBConv(728, 3, strides = 1, expand_ratio = 4, se_ratio = 4, residual = True, momentum = 0.9, epsilon = 1e-5, activation = tf.keras.activations.gelu)(layer99)\n",
    "    layer99 = MBConv(1024, 1, strides = 1, expand_ratio = 4, se_ratio = 4, residual = True, momentum = 0.9, epsilon = 1e-5, activation = tf.keras.activations.gelu)(layer99)\n",
    "\n",
    "    layer350 = tf.keras.layers.concatenate([layer99,x])\n",
    "    #print(layer350.shape)\n",
    "    layer350_ = Conv2D(filters=1024, kernel_size=1, strides=1, padding='same')(layer350)\n",
    "    #print(layer350_.shape)\n",
    "\n",
    "    x = Activation('relu')(layer350_)\n",
    "\n",
    "    x = conv_block(x, [512, 512, 2048], stage=5, block='a')\n",
    "    x = identity_block(x, [512, 512, 2048], stage=5, block='b')\n",
    "    #x = identity_block(x, [512, 512, 2048], stage=5, block='c')\n",
    "\n",
    "    conv_name_base = f'resid_{5}_branch{\"c\"}_'\n",
    "    bn_name_base = f'bn_{5}_branch{\"c\"}_'\n",
    "    y = x\n",
    "    x = Conv2D(512, (1, 1), name=conv_name_base + '2a')(x)\n",
    "    x = BatchNormalization(name=bn_name_base + '2a')(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Conv2D(512, kernel_size=(3, 3), padding='same', name=conv_name_base + '2b')(x)\n",
    "    x = BatchNormalization(name=bn_name_base + '2b')(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Conv2D(2048, (1, 1), name=f'last_layer_conv{2}')(x)\n",
    "    x = BatchNormalization(name=bn_name_base + '2c')(x)\n",
    "\n",
    "    x = Add()([x, y])\n",
    "    feature2 = Activation('relu',name=f'last_layer_act{2}')(x)\n",
    "    #print(feature2.shape)\n",
    "\n",
    "\n",
    "    ############### ResNet50 Transformer #################\n",
    "\n",
    "    trans_out2 = Transformer(feature2)\n",
    "\n",
    "\n",
    "\n",
    "    merged1 = tf.keras.layers.concatenate([feature1,feature2])\n",
    "    merged1 = Conv2D(filters=1024, kernel_size=1, strides=1, padding='same')(merged1)\n",
    "    merged2 = tf.keras.layers.concatenate([trans_out1,trans_out2])\n",
    "    #merged2 = Conv2D(filters=1024, kernel_size=1, strides=1, padding='same')(merged2)\n",
    "    merged = tf.keras.layers.concatenate([merged1,merged2])\n",
    "    #print(merged.shape)\n",
    "\n",
    "    flatten=Flatten()(merged)\n",
    "    dense1=Dense(256,activation='relu')(flatten)\n",
    "    dense1=(Dropout(0.2))(dense1)\n",
    "    dense1=(Dense(128,activation='relu'))(dense1)\n",
    "    dense1=(Dropout(0.3))(dense1)\n",
    "    dense1=(Dense(64,activation='relu'))(dense1)\n",
    "    #(dense1.shape)\n",
    "\n",
    "\n",
    "    model=Dense(8, activation='softmax')(dense1)\n",
    "\n",
    "\n",
    "\n",
    "    M= tf.keras.Model(inputs=input, outputs=model)\n",
    "\n",
    "    M.compile(optimizer = 'adamax', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cbc7e39a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T11:24:53.513932Z",
     "iopub.status.busy": "2024-02-21T11:24:53.513637Z",
     "iopub.status.idle": "2024-02-21T11:25:05.525667Z",
     "shell.execute_reply": "2024-02-21T11:25:05.524899Z"
    },
    "id": "h9qIMKD_KO1M",
    "outputId": "b8b62c2f-77e3-471b-88a4-f13f3219aa9a",
    "papermill": {
     "duration": 12.030769,
     "end_time": "2024-02-21T11:25:05.527959",
     "exception": false,
     "start_time": "2024-02-21T11:24:53.497190",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/xception/xception_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "83683744/83683744 [==============================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "94765736/94765736 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "model = make_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9a370368",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T11:25:05.605129Z",
     "iopub.status.busy": "2024-02-21T11:25:05.604811Z",
     "iopub.status.idle": "2024-02-21T11:25:05.608774Z",
     "shell.execute_reply": "2024-02-21T11:25:05.607937Z"
    },
    "papermill": {
     "duration": 0.024452,
     "end_time": "2024-02-21T11:25:05.610636",
     "exception": false,
     "start_time": "2024-02-21T11:25:05.586184",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e527718c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T11:25:05.647259Z",
     "iopub.status.busy": "2024-02-21T11:25:05.646681Z",
     "iopub.status.idle": "2024-02-21T11:25:05.651020Z",
     "shell.execute_reply": "2024-02-21T11:25:05.650156Z"
    },
    "papermill": {
     "duration": 0.024626,
     "end_time": "2024-02-21T11:25:05.652890",
     "exception": false,
     "start_time": "2024-02-21T11:25:05.628264",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#     n = 8\n",
    "    \n",
    "#     num_features1 = last11_21.shape[1]\n",
    "#     num_features2 = last12_22.shape[1]\n",
    "#     num_features3 = last13_23.shape[1]\n",
    "#     num_features4 = last14_24.shape[1]\n",
    "#     reshaped_output1 = tf.keras.layers.Reshape((n, num_features1 // n))(last11_21)\n",
    "#     reshaped_output2 = tf.keras.layers.Reshape((n, num_features2 // n))(last12_22)\n",
    "#     reshaped_output3 = tf.keras.layers.Reshape((n, num_features3 // n))(last13_23)\n",
    "#     reshaped_output4 = tf.keras.layers.Reshape((n, num_features4 // n))(last14_24)\n",
    "#     lm1 = tf.keras.layers.LSTM(256)(reshaped_output1)\n",
    "#     lm2 = tf.keras.layers.LSTM(256)(reshaped_output2)\n",
    "#     lm3 = tf.keras.layers.LSTM(256)(reshaped_output3)\n",
    "#     lm4 = tf.keras.layers.LSTM(256)(reshaped_output4)\n",
    "    \n",
    "    \n",
    "#     merged1 = tf.keras.layers.concatenate([lm1,lm2])\n",
    "#     merged2 = tf.keras.layers.concatenate([lm3,lm4])  \n",
    "    #merged = tf.keras.layers.concatenate([merged1,merged2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d6a34111",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T11:25:05.689881Z",
     "iopub.status.busy": "2024-02-21T11:25:05.689351Z",
     "iopub.status.idle": "2024-02-21T11:25:08.676267Z",
     "shell.execute_reply": "2024-02-21T11:25:08.675453Z"
    },
    "id": "COSZJ4GlyZKt",
    "outputId": "c6daede4-f7eb-4cb4-944e-b646c1a5743a",
    "papermill": {
     "duration": 3.007622,
     "end_time": "2024-02-21T11:25:08.678671",
     "exception": false,
     "start_time": "2024-02-21T11:25:05.671049",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 files belonging to 8 classes.\n"
     ]
    }
   ],
   "source": [
    "###########  For batch size 128 ###########\n",
    "data = tf.keras.utils.image_dataset_from_directory(directory = '/kaggle/input/kvasir-v2-a-gastrointestinal-tract-dataset',\n",
    "                                                   color_mode = 'rgb',\n",
    "                                                   batch_size = 30,\n",
    "                                                   image_size = (128,128),\n",
    "                                                   shuffle=True,\n",
    "                                                   seed = 2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "41b5abdc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T11:25:08.715656Z",
     "iopub.status.busy": "2024-02-21T11:25:08.715362Z",
     "iopub.status.idle": "2024-02-21T11:25:08.719778Z",
     "shell.execute_reply": "2024-02-21T11:25:08.719062Z"
    },
    "id": "FSOEj816yZH-",
    "outputId": "3de26bc0-dce9-40f7-8296-0dfd8b67b42f",
    "papermill": {
     "duration": 0.025045,
     "end_time": "2024-02-21T11:25:08.721563",
     "exception": false,
     "start_time": "2024-02-21T11:25:08.696518",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "leng = len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a4166343",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T11:25:08.756916Z",
     "iopub.status.busy": "2024-02-21T11:25:08.756651Z",
     "iopub.status.idle": "2024-02-21T11:25:08.760913Z",
     "shell.execute_reply": "2024-02-21T11:25:08.760101Z"
    },
    "id": "lHr0YcqKyZFZ",
    "outputId": "371cce7e-8ed2-41b0-8088-450f29e3d41b",
    "papermill": {
     "duration": 0.023968,
     "end_time": "2024-02-21T11:25:08.762731",
     "exception": false,
     "start_time": "2024-02-21T11:25:08.738763",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_size = int(0.9 * leng)+1\n",
    "val_size = int(0.05 * leng)\n",
    "test_size = int(0.05 * leng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3d4d5333",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T11:25:08.798212Z",
     "iopub.status.busy": "2024-02-21T11:25:08.797940Z",
     "iopub.status.idle": "2024-02-21T11:25:08.812927Z",
     "shell.execute_reply": "2024-02-21T11:25:08.812242Z"
    },
    "id": "v4AAiHZiKetY",
    "papermill": {
     "duration": 0.03486,
     "end_time": "2024-02-21T11:25:08.814875",
     "exception": false,
     "start_time": "2024-02-21T11:25:08.780015",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = data.take(train_size)\n",
    "remaining = data.skip(train_size)\n",
    "val = remaining.take(val_size)\n",
    "remaining2 = remaining.skip(val_size)\n",
    "test = remaining2.take(test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d5fccec6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T11:25:08.850872Z",
     "iopub.status.busy": "2024-02-21T11:25:08.850597Z",
     "iopub.status.idle": "2024-02-21T11:25:08.858958Z",
     "shell.execute_reply": "2024-02-21T11:25:08.858106Z"
    },
    "id": "QPlB9kBzvscu",
    "papermill": {
     "duration": 0.028272,
     "end_time": "2024-02-21T11:25:08.860890",
     "exception": false,
     "start_time": "2024-02-21T11:25:08.832618",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# train batchs = 241, # validate batchs = 13, # test batch = 13\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "267"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"# train batchs = {len(train)}, # validate batchs = {len(val)}, # test batch = {len(test)}\")\n",
    "len(train) + len(val) + len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "70209104",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T11:25:08.898860Z",
     "iopub.status.busy": "2024-02-21T11:25:08.898556Z",
     "iopub.status.idle": "2024-02-21T11:25:44.229154Z",
     "shell.execute_reply": "2024-02-21T11:25:44.228253Z"
    },
    "id": "Y23OjWLQvsZ_",
    "outputId": "ae5f8af0-59ca-4de7-e90b-33c893c3c3c2",
    "papermill": {
     "duration": 35.352901,
     "end_time": "2024-02-21T11:25:44.231593",
     "exception": false,
     "start_time": "2024-02-21T11:25:08.878692",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_iter = test.as_numpy_iterator()\n",
    "test_set = {\"images\":np.empty((0,128,128,3)), \"labels\":np.empty(0)}\n",
    "while True:\n",
    "    try:\n",
    "        batch = test_iter.next()\n",
    "        test_set['images'] = np.concatenate((test_set['images'], batch[0]))\n",
    "        test_set['labels'] = np.concatenate((test_set['labels'], batch[1]))\n",
    "    except:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5d5c5feb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T11:25:44.269238Z",
     "iopub.status.busy": "2024-02-21T11:25:44.268911Z",
     "iopub.status.idle": "2024-02-21T11:25:44.272912Z",
     "shell.execute_reply": "2024-02-21T11:25:44.272018Z"
    },
    "id": "ssi3sU3ryzhk",
    "outputId": "4bc9048e-67e7-4e67-82ff-cda2a5dbd88d",
    "papermill": {
     "duration": 0.0246,
     "end_time": "2024-02-21T11:25:44.274732",
     "exception": false,
     "start_time": "2024-02-21T11:25:44.250132",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_true = test_set['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5d745352",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T11:25:44.313305Z",
     "iopub.status.busy": "2024-02-21T11:25:44.312979Z",
     "iopub.status.idle": "2024-02-21T11:25:44.318957Z",
     "shell.execute_reply": "2024-02-21T11:25:44.317962Z"
    },
    "id": "dW186yBCvsXB",
    "outputId": "a3fe2653-ced6-4aed-d9b0-f73ddb444da8",
    "papermill": {
     "duration": 0.028263,
     "end_time": "2024-02-21T11:25:44.321039",
     "exception": false,
     "start_time": "2024-02-21T11:25:44.292776",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "380"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1415a8fa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T11:25:44.360906Z",
     "iopub.status.busy": "2024-02-21T11:25:44.360598Z",
     "iopub.status.idle": "2024-02-21T15:15:59.951750Z",
     "shell.execute_reply": "2024-02-21T15:15:59.950712Z"
    },
    "id": "g6ruyXgRvsVG",
    "papermill": {
     "duration": 13816.697753,
     "end_time": "2024-02-21T15:16:01.038522",
     "exception": false,
     "start_time": "2024-02-21T11:25:44.340769",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1708514817.307558      87 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "241/241 [==============================] - 403s 1s/step - loss: 1.5721 - accuracy: 0.5770 - val_loss: 1.8858 - val_accuracy: 0.2462\n",
      "Epoch 2/50\n",
      "241/241 [==============================] - 275s 1s/step - loss: 0.5798 - accuracy: 0.7923 - val_loss: 0.7802 - val_accuracy: 0.8385\n",
      "Epoch 3/50\n",
      "241/241 [==============================] - 273s 1s/step - loss: 0.3993 - accuracy: 0.8577 - val_loss: 0.3130 - val_accuracy: 0.9103\n",
      "Epoch 4/50\n",
      "241/241 [==============================] - 278s 1s/step - loss: 0.2974 - accuracy: 0.8990 - val_loss: 0.3323 - val_accuracy: 0.9154\n",
      "Epoch 5/50\n",
      "241/241 [==============================] - 274s 1s/step - loss: 0.2046 - accuracy: 0.9277 - val_loss: 0.2925 - val_accuracy: 0.9179\n",
      "Epoch 6/50\n",
      "241/241 [==============================] - 272s 1s/step - loss: 0.1897 - accuracy: 0.9407 - val_loss: 0.2744 - val_accuracy: 0.9077\n",
      "Epoch 7/50\n",
      "241/241 [==============================] - 272s 1s/step - loss: 0.1078 - accuracy: 0.9657 - val_loss: 0.3565 - val_accuracy: 0.9385\n",
      "Epoch 8/50\n",
      "241/241 [==============================] - 272s 1s/step - loss: 0.1115 - accuracy: 0.9703 - val_loss: 0.3629 - val_accuracy: 0.9359\n",
      "Epoch 9/50\n",
      "241/241 [==============================] - 272s 1s/step - loss: 0.0796 - accuracy: 0.9793 - val_loss: 0.5029 - val_accuracy: 0.9128\n",
      "Epoch 10/50\n",
      "241/241 [==============================] - 272s 1s/step - loss: 0.0493 - accuracy: 0.9881 - val_loss: 0.3539 - val_accuracy: 0.9410\n",
      "Epoch 11/50\n",
      "241/241 [==============================] - 272s 1s/step - loss: 0.0448 - accuracy: 0.9869 - val_loss: 0.2738 - val_accuracy: 0.9385\n",
      "Epoch 12/50\n",
      "241/241 [==============================] - 272s 1s/step - loss: 0.0556 - accuracy: 0.9878 - val_loss: 0.5634 - val_accuracy: 0.9359\n",
      "Epoch 13/50\n",
      "241/241 [==============================] - 272s 1s/step - loss: 0.0605 - accuracy: 0.9864 - val_loss: 0.5489 - val_accuracy: 0.9282\n",
      "Epoch 14/50\n",
      "241/241 [==============================] - 272s 1s/step - loss: 0.0311 - accuracy: 0.9913 - val_loss: 0.3727 - val_accuracy: 0.9538\n",
      "Epoch 15/50\n",
      "241/241 [==============================] - 273s 1s/step - loss: 0.0495 - accuracy: 0.9902 - val_loss: 0.4931 - val_accuracy: 0.9282\n",
      "Epoch 16/50\n",
      "241/241 [==============================] - 272s 1s/step - loss: 0.0111 - accuracy: 0.9970 - val_loss: 0.6959 - val_accuracy: 0.9282\n",
      "Epoch 17/50\n",
      "241/241 [==============================] - 272s 1s/step - loss: 0.0069 - accuracy: 0.9983 - val_loss: 0.5754 - val_accuracy: 0.9256\n",
      "Epoch 18/50\n",
      "241/241 [==============================] - 272s 1s/step - loss: 0.0198 - accuracy: 0.9957 - val_loss: 0.4573 - val_accuracy: 0.9308\n",
      "Epoch 19/50\n",
      "241/241 [==============================] - 272s 1s/step - loss: 0.0366 - accuracy: 0.9935 - val_loss: 0.5345 - val_accuracy: 0.9436\n",
      "Epoch 20/50\n",
      "241/241 [==============================] - 272s 1s/step - loss: 0.0272 - accuracy: 0.9947 - val_loss: 0.5230 - val_accuracy: 0.9385\n",
      "Epoch 21/50\n",
      "241/241 [==============================] - 272s 1s/step - loss: 0.0281 - accuracy: 0.9949 - val_loss: 0.6815 - val_accuracy: 0.9359\n",
      "Epoch 22/50\n",
      "241/241 [==============================] - 273s 1s/step - loss: 0.0213 - accuracy: 0.9936 - val_loss: 0.7183 - val_accuracy: 0.9410\n",
      "Epoch 23/50\n",
      "241/241 [==============================] - 272s 1s/step - loss: 0.0128 - accuracy: 0.9971 - val_loss: 0.7492 - val_accuracy: 0.9487\n",
      "Epoch 24/50\n",
      "241/241 [==============================] - 272s 1s/step - loss: 0.0160 - accuracy: 0.9961 - val_loss: 0.5782 - val_accuracy: 0.9385\n",
      "Epoch 25/50\n",
      "241/241 [==============================] - 272s 1s/step - loss: 0.0404 - accuracy: 0.9917 - val_loss: 0.5623 - val_accuracy: 0.9333\n",
      "Epoch 26/50\n",
      "241/241 [==============================] - 272s 1s/step - loss: 0.0227 - accuracy: 0.9961 - val_loss: 0.4333 - val_accuracy: 0.9487\n",
      "Epoch 27/50\n",
      "241/241 [==============================] - 272s 1s/step - loss: 0.0277 - accuracy: 0.9952 - val_loss: 0.7713 - val_accuracy: 0.9308\n",
      "Epoch 28/50\n",
      "241/241 [==============================] - 272s 1s/step - loss: 0.0127 - accuracy: 0.9964 - val_loss: 0.8440 - val_accuracy: 0.9410\n",
      "Epoch 29/50\n",
      "241/241 [==============================] - 272s 1s/step - loss: 0.0223 - accuracy: 0.9959 - val_loss: 0.7559 - val_accuracy: 0.9359\n",
      "Epoch 30/50\n",
      "241/241 [==============================] - 272s 1s/step - loss: 0.0132 - accuracy: 0.9970 - val_loss: 0.8438 - val_accuracy: 0.9231\n",
      "Epoch 31/50\n",
      "241/241 [==============================] - 273s 1s/step - loss: 0.0162 - accuracy: 0.9961 - val_loss: 0.7825 - val_accuracy: 0.9385\n",
      "Epoch 32/50\n",
      "241/241 [==============================] - 272s 1s/step - loss: 0.0120 - accuracy: 0.9976 - val_loss: 0.8088 - val_accuracy: 0.9487\n",
      "Epoch 33/50\n",
      "241/241 [==============================] - 272s 1s/step - loss: 0.0218 - accuracy: 0.9953 - val_loss: 0.9686 - val_accuracy: 0.9231\n",
      "Epoch 34/50\n",
      "241/241 [==============================] - 273s 1s/step - loss: 0.0173 - accuracy: 0.9971 - val_loss: 1.2716 - val_accuracy: 0.9205\n",
      "Epoch 35/50\n",
      "241/241 [==============================] - 272s 1s/step - loss: 0.0308 - accuracy: 0.9949 - val_loss: 1.0422 - val_accuracy: 0.9205\n",
      "Epoch 36/50\n",
      "241/241 [==============================] - 286s 1s/step - loss: 0.0123 - accuracy: 0.9975 - val_loss: 0.8857 - val_accuracy: 0.9333\n",
      "Epoch 37/50\n",
      "241/241 [==============================] - 273s 1s/step - loss: 0.0066 - accuracy: 0.9982 - val_loss: 1.0198 - val_accuracy: 0.9385\n",
      "Epoch 38/50\n",
      "241/241 [==============================] - 273s 1s/step - loss: 0.0211 - accuracy: 0.9964 - val_loss: 1.0005 - val_accuracy: 0.9128\n",
      "Epoch 39/50\n",
      "241/241 [==============================] - 272s 1s/step - loss: 0.0072 - accuracy: 0.9985 - val_loss: 0.6656 - val_accuracy: 0.9410\n",
      "Epoch 40/50\n",
      "241/241 [==============================] - 272s 1s/step - loss: 0.0160 - accuracy: 0.9956 - val_loss: 0.5503 - val_accuracy: 0.9487\n",
      "Epoch 41/50\n",
      "241/241 [==============================] - 275s 1s/step - loss: 0.0081 - accuracy: 0.9988 - val_loss: 0.6802 - val_accuracy: 0.9308\n",
      "Epoch 42/50\n",
      "241/241 [==============================] - 272s 1s/step - loss: 0.0097 - accuracy: 0.9976 - val_loss: 0.7893 - val_accuracy: 0.9282\n",
      "Epoch 43/50\n",
      "241/241 [==============================] - 275s 1s/step - loss: 0.0208 - accuracy: 0.9968 - val_loss: 0.8003 - val_accuracy: 0.9436\n",
      "Epoch 44/50\n",
      "241/241 [==============================] - 272s 1s/step - loss: 0.0169 - accuracy: 0.9976 - val_loss: 0.7623 - val_accuracy: 0.9462\n",
      "Epoch 45/50\n",
      "241/241 [==============================] - 272s 1s/step - loss: 0.0117 - accuracy: 0.9976 - val_loss: 0.6667 - val_accuracy: 0.9462\n",
      "Epoch 46/50\n",
      "241/241 [==============================] - 272s 1s/step - loss: 0.0125 - accuracy: 0.9981 - val_loss: 0.5482 - val_accuracy: 0.9410\n",
      "Epoch 47/50\n",
      "241/241 [==============================] - 272s 1s/step - loss: 0.0387 - accuracy: 0.9953 - val_loss: 0.7734 - val_accuracy: 0.9308\n",
      "Epoch 48/50\n",
      "241/241 [==============================] - 273s 1s/step - loss: 0.0155 - accuracy: 0.9976 - val_loss: 0.6812 - val_accuracy: 0.9462\n",
      "Epoch 49/50\n",
      "241/241 [==============================] - 272s 1s/step - loss: 0.0123 - accuracy: 0.9981 - val_loss: 0.8635 - val_accuracy: 0.9410\n",
      "Epoch 50/50\n",
      "241/241 [==============================] - 272s 1s/step - loss: 0.0087 - accuracy: 0.9981 - val_loss: 1.2452 - val_accuracy: 0.9256\n"
     ]
    }
   ],
   "source": [
    "from keras import callbacks\n",
    "early_stop = callbacks.EarlyStopping(\n",
    "        monitor=\"val_accuracy\",\n",
    "        patience=15,\n",
    "        verbose=1,\n",
    "        mode=\"max\",\n",
    "        restore_best_weights=True,\n",
    "     )\n",
    "\n",
    "history = model.fit(\n",
    "    train,\n",
    "    validation_data = val,\n",
    "    epochs = 50,\n",
    "    #steps_per_epoch=20,\n",
    "    #callbacks=[early_stop]\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bb2c19a5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T15:16:03.012995Z",
     "iopub.status.busy": "2024-02-21T15:16:03.011947Z",
     "iopub.status.idle": "2024-02-21T15:17:10.418731Z",
     "shell.execute_reply": "2024-02-21T15:17:10.417638Z"
    },
    "id": "8XAHanMWvsSX",
    "outputId": "e154a6a5-87ea-49f2-c663-8efaef54658a",
    "papermill": {
     "duration": 68.398762,
     "end_time": "2024-02-21T15:17:10.421095",
     "exception": false,
     "start_time": "2024-02-21T15:16:02.022333",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 36s 970ms/step - loss: 1.4676 - accuracy: 0.8947\n",
      "12/12 [==============================] - 31s 1s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/kaggle/working/model_Proposed_prediction.csv'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score, cohen_kappa_score, confusion_matrix\n",
    "import csv\n",
    "import shutil\n",
    "from sklearn.metrics import recall_score, precision_score\n",
    "import numpy as np\n",
    "\n",
    "f1_scores = []\n",
    "sn_scores = []\n",
    "ppv_scores = []\n",
    "test_accuracies = []\n",
    "test_loss = []\n",
    "kappa_scores = []\n",
    "specificity_scores = []\n",
    "\n",
    "test_results = model.evaluate(test)\n",
    "\n",
    "# Extract the evaluation metrics\n",
    "test_loss_value = test_results[0]\n",
    "test_accuracy = test_results[1]\n",
    "\n",
    "# Calculate additional evaluation metrics based on predictions and ground truth labels\n",
    "# test_predictions = model.predict(test)\n",
    "# test_labels = [label for _, label in test]\n",
    "\n",
    "# Perform necessary calculations for the evaluation metrics (e.g., F1-score, Sensitivity, PPV, Kappa)\n",
    "y_pred = model.predict(test_set['images'])\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "f1_score1 = f1_score(y_true, y_pred_classes, average='macro')\n",
    "sn = recall_score(y_true, y_pred_classes, average='macro')\n",
    "ppv = precision_score(y_true, y_pred_classes, average='macro')\n",
    "kappa = cohen_kappa_score(y_true, y_pred_classes)\n",
    "conf_matrix = confusion_matrix(y_true, y_pred_classes)\n",
    "\n",
    "# Extract TN (True Negatives) and FP (False Positives) from the confusion matrix\n",
    "TN = conf_matrix[0, 0]\n",
    "FP = conf_matrix[0, 1]\n",
    "\n",
    "# Calculate specificity\n",
    "specificity = TN / (TN + FP)\n",
    "\n",
    "# Store the evaluation metrics for the current fold\n",
    "f1_scores.append(f1_score1)\n",
    "sn_scores.append(sn)\n",
    "ppv_scores.append(ppv)\n",
    "test_accuracies.append(test_accuracy)\n",
    "test_loss.append(test_loss_value)\n",
    "kappa_scores.append(kappa)\n",
    "specificity_scores.append(specificity)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#history = model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=10)\n",
    "\n",
    "# Get the accuracy, loss, recall, and sensitivity for each epoch\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    " # convert probabilities to class labels\n",
    "# recall = recall_score(y_true, y_pred_classes, average='macro')\n",
    "# precision = precision_score(y_true, y_pred_classes, average='macro')\n",
    "\n",
    "# Write the accuracy, loss, recall, and sensitivity to a CSV file\n",
    "output_file_path = '/kaggle/working/accuracy_loss_recall_sensitivity.csv'\n",
    "\n",
    "with open(output_file_path, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Epoch', 'Train-Accuracy', 'Validation Accuracy', 'Train-Loss', 'Validation Loss', 'F1-Score', 'Sensitivity','Precition','Test-Acc','Test-Loss','Kappa','Specificity'])\n",
    "        for j in range(len(acc)):\n",
    "            writer.writerow([j+1, acc[j], val_acc[j], loss[j], val_loss[j], f1_scores, sn_scores,ppv_scores,test_accuracies,test_loss,kappa_scores,specificity_scores])\n",
    "\n",
    "destination_file_path = '/kaggle/working/model_Proposed.csv'\n",
    "shutil.copy(output_file_path, destination_file_path)    \n",
    "\n",
    "\n",
    "output_file_path2 = '/kaggle/working/accuracy_loss_recall_sensitivity_pred.csv'\n",
    "with open(output_file_path2, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Number','Y_true','Y_pred'])\n",
    "    for j in range(len(y_true)):\n",
    "        writer.writerow([j+1,y_true[j],y_pred_classes[j]])\n",
    "\n",
    "destination_file_path2 = '/kaggle/working/model_Proposed_prediction.csv'\n",
    "shutil.copy(output_file_path2, destination_file_path2)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dcf98da5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T15:17:12.405009Z",
     "iopub.status.busy": "2024-02-21T15:17:12.404088Z",
     "iopub.status.idle": "2024-02-21T15:17:17.647708Z",
     "shell.execute_reply": "2024-02-21T15:17:17.646650Z"
    },
    "papermill": {
     "duration": 6.224578,
     "end_time": "2024-02-21T15:17:17.650303",
     "exception": false,
     "start_time": "2024-02-21T15:17:11.425725",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.save('/kaggle/working/Proposed_Model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c59b2cf7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T15:17:19.682965Z",
     "iopub.status.busy": "2024-02-21T15:17:19.682357Z",
     "iopub.status.idle": "2024-02-21T15:17:19.688205Z",
     "shell.execute_reply": "2024-02-21T15:17:19.687319Z"
    },
    "id": "dpwDvNHvvsPr",
    "papermill": {
     "duration": 0.986585,
     "end_time": "2024-02-21T15:17:19.690157",
     "exception": false,
     "start_time": "2024-02-21T15:17:18.703572",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# n_folds = 5  # number of folds here\n",
    "\n",
    "# train_size = int(0.9 * leng) +1\n",
    "# #val_size = int(0.05 * leng)\n",
    "# test_size = int(0.1 * leng)\n",
    "# train = data.take(train_size)\n",
    "# # remaining = data.skip(train_size)\n",
    "# # val = remaining.take(val_size)\n",
    "# remaining2 = data.skip(train_size)\n",
    "# test = remaining2.take(test_size)\n",
    "\n",
    "\n",
    "\n",
    "# # calculate the size of each fold\n",
    "# fold_size = train.cardinality().numpy() // n_folds\n",
    "\n",
    "# # initialize lists to store the folds and remainders\n",
    "# folds = []\n",
    "# remainders = [train]\n",
    "\n",
    "# # loop over the number of folds and create each fold\n",
    "# for i in range(n_folds):\n",
    "#     # take the first `fold_size` samples from the current remainder to create a fold\n",
    "#     fold = remainders[-1].take(fold_size)\n",
    "    \n",
    "#     # append the fold to the list of folds\n",
    "#     folds.append(fold)\n",
    "    \n",
    "#     # skip the samples in the fold to create the next remainder\n",
    "#     remainder = remainders[-1].skip(fold_size)\n",
    "    \n",
    "#     # append the remainder to the list of remainders\n",
    "#     remainders.append(remainder)\n",
    "\n",
    "\n",
    "\n",
    "# # initialize lists to store the training and validation losses for each fold\n",
    "# # f1_scores = []\n",
    "# # sn_scores = []\n",
    "# # ppv_scores = []\n",
    "# # test_accuracies = []\n",
    "# # test_loss = []\n",
    "# # kappa_scores = []\n",
    "\n",
    "\n",
    "# ################## Test Itera ####################\n",
    "\n",
    "# test_iter = test.as_numpy_iterator()\n",
    "\n",
    "# test_set = {\"images\":np.empty((0,128,128,3)), \"labels\":np.empty(0)}\n",
    "# while True:\n",
    "#     try:\n",
    "#         batch = test_iter.next()\n",
    "#         test_set['images'] = np.concatenate((test_set['images'], batch[0]))\n",
    "#         test_set['labels'] = np.concatenate((test_set['labels'], batch[1]))\n",
    "#     except:\n",
    "#         break\n",
    "\n",
    "# y_true = test_set['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8397ec82",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T15:17:21.780873Z",
     "iopub.status.busy": "2024-02-21T15:17:21.780314Z",
     "iopub.status.idle": "2024-02-21T15:17:21.793868Z",
     "shell.execute_reply": "2024-02-21T15:17:21.792879Z"
    },
    "id": "SJFE2OiSvsNH",
    "papermill": {
     "duration": 1.12748,
     "end_time": "2024-02-21T15:17:21.796786",
     "exception": false,
     "start_time": "2024-02-21T15:17:20.669306",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from sklearn.metrics import f1_score, recall_score, precision_score, cohen_kappa_score, confusion_matrix\n",
    "# import csv\n",
    "# import shutil\n",
    "# from sklearn.metrics import recall_score, precision_score\n",
    "# import numpy as np\n",
    "# for i in range(0,5):\n",
    "\n",
    "#     model = make_model()\n",
    "#     f1_scores = []\n",
    "#     sn_scores = []\n",
    "#     ppv_scores = []\n",
    "#     test_accuracies = []\n",
    "#     test_loss = []\n",
    "#     kappa_scores = []\n",
    "#     specificity_scores = []\n",
    "#     print(\"Fold: \",i)\n",
    "#     # create a list of folds to use as the training set\n",
    "#     train_folds = [f for j, f in enumerate(folds) if j != i]\n",
    "#     #val_folds = [f for j, f in enumerate(vals) if j != i]\n",
    "#     # concatenate the training folds into a single dataset\n",
    "#     train_data = tf.data.experimental.sample_from_datasets(train_folds)\n",
    "#     #val_data = tf.data.experimental.sample_from_datasets(val_folds)\n",
    "#     # train the model using the training data and the current fold as the validation data\n",
    "#     history = model.fit(train_data, epochs = 40, validation_data=folds[i])\n",
    "    \n",
    "#     # Evaluate the model on the test set\n",
    "#     test_results = model.evaluate(test)\n",
    "\n",
    "#     # Extract the evaluation metrics\n",
    "#     test_loss_value = test_results[0]\n",
    "#     test_accuracy = test_results[1]\n",
    "\n",
    "#     # Calculate additional evaluation metrics based on predictions and ground truth labels\n",
    "#     # test_predictions = model.predict(test)\n",
    "#     # test_labels = [label for _, label in test]\n",
    "\n",
    "#     # Perform necessary calculations for the evaluation metrics (e.g., F1-score, Sensitivity, PPV, Kappa)\n",
    "#     y_pred = model.predict(test_set['images'])\n",
    "#     y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "#     f1_score1 = f1_score(y_true, y_pred_classes, average='macro')\n",
    "#     sn = recall_score(y_true, y_pred_classes, average='macro')\n",
    "#     ppv = precision_score(y_true, y_pred_classes, average='macro')\n",
    "#     kappa = cohen_kappa_score(y_true, y_pred_classes)\n",
    "#     conf_matrix = confusion_matrix(y_true, y_pred_classes)\n",
    "\n",
    "#     # Extract TN (True Negatives) and FP (False Positives) from the confusion matrix\n",
    "#     TN = conf_matrix[0, 0]\n",
    "#     FP = conf_matrix[0, 1]\n",
    "\n",
    "#     # Calculate specificity\n",
    "#     specificity = TN / (TN + FP)\n",
    "\n",
    "#     # Store the evaluation metrics for the current fold\n",
    "#     f1_scores.append(f1_score1)\n",
    "#     sn_scores.append(sn)\n",
    "#     ppv_scores.append(ppv)\n",
    "#     test_accuracies.append(test_accuracy)\n",
    "#     test_loss.append(test_loss_value)\n",
    "#     kappa_scores.append(kappa)\n",
    "#     specificity_scores.append(specificity)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     #history = model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=10)\n",
    "\n",
    "#     # Get the accuracy, loss, recall, and sensitivity for each epoch\n",
    "#     acc = history.history['accuracy']\n",
    "#     val_acc = history.history['val_accuracy']\n",
    "#     loss = history.history['loss']\n",
    "#     val_loss = history.history['val_loss']\n",
    "    \n",
    "#      # convert probabilities to class labels\n",
    "#     # recall = recall_score(y_true, y_pred_classes, average='macro')\n",
    "#     # precision = precision_score(y_true, y_pred_classes, average='macro')\n",
    "\n",
    "#     # Write the accuracy, loss, recall, and sensitivity to a CSV file\n",
    "#     output_file_path = '/kaggle/working/accuracy_loss_recall_sensitivity.csv'\n",
    "\n",
    "#     with open(output_file_path, mode='w', newline='') as file:\n",
    "#             writer = csv.writer(file)\n",
    "#             writer.writerow(['Epoch', 'Train-Accuracy', 'Validation Accuracy', 'Train-Loss', 'Validation Loss', 'F1-Score', 'Sensitivity','Precition','Test-Acc','Test-Loss','Kappa','Specificity'])\n",
    "#             for j in range(len(acc)):\n",
    "#                 writer.writerow([j+1, acc[j], val_acc[j], loss[j], val_loss[j], f1_scores, sn_scores,ppv_scores,test_accuracies,test_loss,kappa_scores,specificity_scores])\n",
    "\n",
    "#     destination_file_path = '/kaggle/working/Proposed_Model_Fold ' + str(i) + '.csv'\n",
    "#     shutil.copy(output_file_path, destination_file_path)    \n",
    "\n",
    "#     output_file_path_pred = '/kaggle/working/accuracy_loss_recall_sensitivity_predict.csv'\n",
    "\n",
    "#     with open(output_file_path_pred, mode='w', newline='') as file:\n",
    "#             writer = csv.writer(file)\n",
    "#             writer.writerow(['Number','Y_true','Y_pred'])\n",
    "#             for j in range(len(y_true)):\n",
    "#                 writer.writerow([j+1,y_true[j],y_pred_classes[j]])\n",
    "\n",
    "#     destination_file_path_pred = '/kaggle/working/Proposed_Prediction_Fold ' + str(i) + '.csv'\n",
    "#     shutil.copy(output_file_path_pred, destination_file_path_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61bfc82",
   "metadata": {
    "id": "O0h0GzflvsKW",
    "papermill": {
     "duration": 1.000702,
     "end_time": "2024-02-21T15:17:23.896561",
     "exception": false,
     "start_time": "2024-02-21T15:17:22.895859",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9df642",
   "metadata": {
    "id": "IjEMhiQ_vsH3",
    "outputId": "55eeab84-4845-468d-b4f5-599f8222b5fc",
    "papermill": {
     "duration": 1.054149,
     "end_time": "2024-02-21T15:17:26.040677",
     "exception": false,
     "start_time": "2024-02-21T15:17:24.986528",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536d6765",
   "metadata": {
    "id": "3RRWmFSQHo0d",
    "papermill": {
     "duration": 1.086959,
     "end_time": "2024-02-21T15:17:28.154920",
     "exception": false,
     "start_time": "2024-02-21T15:17:27.067961",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0535e6",
   "metadata": {
    "papermill": {
     "duration": 1.007039,
     "end_time": "2024-02-21T15:17:30.157031",
     "exception": false,
     "start_time": "2024-02-21T15:17:29.149992",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f752e92b",
   "metadata": {
    "papermill": {
     "duration": 0.985051,
     "end_time": "2024-02-21T15:17:32.234462",
     "exception": false,
     "start_time": "2024-02-21T15:17:31.249411",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab701f6",
   "metadata": {
    "papermill": {
     "duration": 1.090519,
     "end_time": "2024-02-21T15:17:34.298399",
     "exception": false,
     "start_time": "2024-02-21T15:17:33.207880",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acc96b6",
   "metadata": {
    "papermill": {
     "duration": 0.976766,
     "end_time": "2024-02-21T15:17:36.247907",
     "exception": false,
     "start_time": "2024-02-21T15:17:35.271141",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138cd8c1",
   "metadata": {
    "papermill": {
     "duration": 0.982262,
     "end_time": "2024-02-21T15:17:38.279297",
     "exception": false,
     "start_time": "2024-02-21T15:17:37.297035",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc572cba",
   "metadata": {
    "papermill": {
     "duration": 1.043521,
     "end_time": "2024-02-21T15:17:40.292142",
     "exception": false,
     "start_time": "2024-02-21T15:17:39.248621",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6774cdd",
   "metadata": {
    "papermill": {
     "duration": 0.975625,
     "end_time": "2024-02-21T15:17:42.238047",
     "exception": false,
     "start_time": "2024-02-21T15:17:41.262422",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e637c72b",
   "metadata": {
    "papermill": {
     "duration": 0.996249,
     "end_time": "2024-02-21T15:17:44.300844",
     "exception": false,
     "start_time": "2024-02-21T15:17:43.304595",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829a62ae",
   "metadata": {
    "papermill": {
     "duration": 0.997777,
     "end_time": "2024-02-21T15:17:46.264852",
     "exception": false,
     "start_time": "2024-02-21T15:17:45.267075",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d540da9",
   "metadata": {
    "papermill": {
     "duration": 0.984075,
     "end_time": "2024-02-21T15:17:48.350288",
     "exception": false,
     "start_time": "2024-02-21T15:17:47.366213",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 1297165,
     "sourceId": 2161046,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30648,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 14046.254655,
   "end_time": "2024-02-21T15:17:53.310756",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-02-21T11:23:47.056101",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
